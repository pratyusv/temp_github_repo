---
layout: post
title: Design Data Intensive Applications
date: 2022-01-03 00:12:00-0400
# description: Instance
categories: ['System Design']
---


### Loads

Parameters for load can be:
* Requests per second to a web server
* ratio of reads/write per seconds to database
* number of simultanous users in a chat room
* hit rate on cache 


---

### Database

The read/write performance of a database heavily depends on the data structure which it uses to store data. We consider that every record is of key/value type and evaluate different types of data structures used by databases.

#### Log based (Append Only)
Some databases use the append only mechanism. They use log file that is appended on write operation. They have a very good performance for write operation but are very slow for read. For a read operation, the entire log has to be traversed to find the value. 

There is an issue of disk space also with log based database as every write to the same key will also increase the size. One possible solution for this problem is `Compaction`. There can be a compaction service running that can go through the logs and entries for the same key. The service can then retain the latest change and discard all the previous changes resulting in efficiant usage of disk space.

#### Compaction 

The compaction can be performed on a seperate thread without disturbing the read/write operation of database. Each log file can be viewd as a data segement. We never update any exisiting data segement but create new ones. 

1. Existing data segments can be loaded in memory hash map.
2. Same keys from different segments can be looked up efficently (using hashmap) and merged.
3. We create new segments while combining the keys. Meanwhile the existing read/write operation can be performed using the existing data segment.
4. Once the merging is completed, we can switch to the new data segment and perform read/write operations from the new segment.

#### Index 
Relational databases maintain indexes on the key. They have high speed reads, but writes are very slow as every write will update the index.

* Hash Index: Some database maintain a hash map in memory to store the data. The primary condition of these type of database is to have limited number of keys that can fit into memory.

