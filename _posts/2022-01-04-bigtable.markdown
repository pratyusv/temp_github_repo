---
layout: post
title: BigTable
date: 2022-01-04 00:12:00-0400
description: Wide column database
categories: ['Distributed Systems Components']
---

It is a **Consistent, Partial Fault Tolerence** type datatabase with strict consistent read/writes. It is suitable for large storage where each row is less than 10MB. It is used to store data for one of the types:
1. Time Series data: chronological data
2. Constant stream of writes

> Use BigTable: Messaging App, Web Crawler, Analytics, Google Earth

**`Hbase`** and **`HyperTable`** databases are based on BigTable.

---

#### Data Model

A RDBMS database stores data as a two dimensional entity, while big table stores data as four dimenstional identity.

1. Row Key: uniquely identifies row
2. Column Family: Represents group of columns
3. Column qualifier: column name
4. Timestamp

<div>
    <img src="{{ site.baseurl }}/assets/img/BigTableDataModel.png">
</div>


<div>
    <img src="{{ site.baseurl }}/assets/img/BigTableRow.png">
</div>


Here the row key is `com.cnn.www`. Content and anchor are column family. Anchor has two column qualifier: cnnsi.com, my.look.ca. The content column has entries at multiple timestamp.

The data is indexed by

> (row_key: string, column_name: string, timestamp: int64) $\rightarrow$ cell contents (string)

---

#### Rows
1. Rows keys are arbitrary string with size of 64KB.
2. Every read or write to a row is atomic.
3. Rows are maintained in a lexicographic order by row key

----
#### Blocks

BigTable uses **`GFS`** to store Logs and data files. Internally the data is stored as **`SSTables`**. An *SSTabe* provides persistent, ordered immutable map from key to values.  

Internally each SSTable contains sequence of blocks (64KB size). A *block index* stored at the end of the SSTable, is used to locate the blocks. The *block index* is loaded into the memory when SSTable is opened.

To find a block: binary search is performed on the *block index* to locate the block offset. Then a single seek is performed, to read the block. 

----

#### Implementation

A single instance of BigTable is called **`Cluster`**. Each cluster can store multiple **`Tables`**. Each table is split into multiple **`Tablets`**.

1. Tablet store contigous rows
2. Each tablet is assigned to a **`Tablet Server`**

BigTable has three major components:
1. library that is linked to every client
2. Single master server
3. Many tablet server : can be dynamically added or removed from the cluster

<p>Master</p>
Master is responsible for assigning tablets to tablet servers, detecting the addition and expiration of tablet servers, balancing tablet-server load, etc. It also handles schema changes such as table and column family creations.

`Client always does not have to move through master, thereby the load on the master is low. Clients can directly communicate with the Tablet server.`

<p>Tablet Server</p>
Each tablet server manages multiple tablets (10 to 10000). It handles read/write requests to the tablets that it has loaded, and also splits tablets when they grow too large.



---

#### Tablet Location    

Three level-heirarchy analogous to B+ tree is used to store tablet information.

<div>
    <img src="{{ site.baseurl }}/assets/img/TabletLocation.png">
</div>


A chubby file contains the location of the `root tablet`. The root tablet has a special **`METADATA`** table that stores the Tablet locations.  It stores the stores the location of a tablet under a row key that is encoding of *`tablet's table identifier and end row`*

```json
METADATA:
    Key: table id + end row
    Value: tablet server location
```
* Root Metadata tablet: This tablet is undivisible and store the location of all the other metadata tablets.
* Other Metadata tablets: They store the location of user metadata tablets.

The client caches the information about the tablet location. If the tablet location is unknown or changed, then it recursively moves up the tablet location heirarchy.

----

#### Tablet Assignment

Each tablet is assigned to one tablet server.

Master keeps track of live tablet servers, and current assignment of tablets to tablet servers as well as unassigned tablets. BigTable uses *`Chubby`* to keep tack of tablet servers.

1. When a tablet server starts, it creates and acquires an exclusive lock on, a uniquely-named file in the sepcific chubby directory.
2. The master moniters this directory to keep track of the tablet servers.
3. A tablet server stops serving the tablets if it looses its exclusive lock.
4. A tablet server will try to reaquire the lock as long as the file exists. If the file does not exists then the tablet server will kill itself
5. Master is responsible for detecting when a tablet server is no longer serving its tablets, and for reassigning those tablets to other servers. 
    * To achieve this, master periodically asks the tablet server for the status of its lock.
    * If the tablet server reports that it has lost it locks or master is unable to reach the server, Master attempts to acquire exclusive lock on the server's file. If master is able to acquire the lock and Chubby is live, the Master can conclude that either the server is dead or server is having trouble reaching the chubby. 
    * Master delete's the server file and moves the associated tablets to unassigned.


To ensure that master and chubby are not vulnerable to networking issues, Master kills itself when the Chubby session expires. Cluster Management System restarts the master. On start up, master performes the following steps:

1. Master grabs unique *master* lock in Chubby, preventing concurrent master installation
2. Master scans the servers directory of Chubby to find live servers
3. Master communicates with every live tablet servers to discover what tablet are already assigned to each server.
4. Master scans the METADATA table to learn the set of tablets. While scanning if it encounters any tablet not assigned to any tablet server, master adds it to the list of unassigned.

----

#### Tablet Serving

The persistant state of table are stored in GFS. Updates are commited to a commit log that stores redo records. Of these updates, the recently commited ones are stored in memory in a sorted buffer called `memtable`; older updates are stored in `SSTables`.

<div>
    <img src="{{ site.baseurl }}/assets/img/TabletStorage.png">
</div>

<p>Tablet recovery</p>

1. To recover a tablet, the server reads its metadata from METADATA table.
2. This metadata contains the list of SSTables that comprise a tablet and a set of redo points, which are pointers into any commit logs that may contain data for the tablet.
3. The server reads the indices of the SSTables into memory and reconstructs the memtable by applying all of the updates that have committed since the redo points.


----

#### Write Request
1. The write request arriving at the tablet server is checked if it is well formed and writer is authorised to write.
2. The mutation is written to the commit log in GFS that store the redo logs.
3. Once the mutation is committed to the commit-log, its content are stored in a sorted memory called `MemTable`
4. After inserting into MemTable, ack is sent to the client.
5. Periodically MemTables are flushed to SSTables, and SSTables are merged during compaction

<div>
    <img src="{{ site.baseurl }}/assets/img/BigTableWrite.png">
</div>

#### Read Request
1. When a read request arrives at the tablet server, it is checked for well-formedness.
2. A valid read request is executed on a merged sequence of SSTable and MemTable. Since both are lexicographically sorted the view can be created efficiently.
